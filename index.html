<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhengxiao Han</title>

    <meta name="author" content="Zhengxiao Han">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Zhengxiao Han
                  </p>
                  <p>I am an <a href="https://www.mccormick.northwestern.edu/robotics/">MSR</a> student at Northwestern Univeristy. 
                    I received my Bachelor's degree in 2023 at <a href="https://english.buct.edu.cn/main.htm">Beijing University of Chemical Technology</a>.
                  </p>
                  <p>
                    I spent one year as a Research Assistant at 
                    <a href="https://zsdonghao.github.io/">PKU-Agibot (Êô∫ÂÖÉÊú∫Âô®‰∫∫) Lab</a>, Peking University, 
                    advised by Prof. <a href="https://zsdonghao.github.io/"> Hao Dong</a>.
                  </p>
                  <p>
                    I spent two years as a Research Intern at <a href="https://air.tsinghua.edu.cn/en/">DISCOVER Lab</a>, Tsinghua University, 
                    advised by <a href="https://x.com/arx_zhang?lang=en">Xinliang Zhang</a>, Prof. <a href="https://sites.google.com/view/fromandto">Hao Zhao</a> and 
                    Prof. <a href="https://www.innovatorsunder35.com/the-list/guyue-zhou/">Guyue Zhou</a>. During this time, I collaborated closely with 
                    <a href="https://arx-x.com/">ARX (ÊñπËàüÊó†Èôê)</a> and <a href="https://airbots.online/">DISCOVER Robotics (Ê±Ç‰πãÁßëÊäÄ)</a> during their formative stages, 
                    prior to their official founding.
                  </p>
                  <p>
                    Proudly, I am a founding contributor to 
                    <a href="https://www.kickstarter.com/projects/336477435/mini-pupper-open-sourceros-robot-dog-kit"> Mini Pupper</a>, 
                    an open-source quadruped robot.
                  </p>
                  <!-- <p>
                    I am <b style="color: #FF0000;">actively</b> seeking PhD positions in Robotics for 2025 Fall, 2026 Spring and 2026 Fall.
                  </p> -->
                  <p style="text-align:center">
                    <a href="https://www.linkedin.com/in/purimagination/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/0nhc/">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://x.com/serious0nhc">X</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=m0KhpYUAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="mailto:hanzx@u.northwestern.edu">Email</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="index.html">
                    <img 
                    style="width:70%;max-width:100%;object-fit: cover; border-radius: 50%;" 
                    alt="profile photo" 
                    src="images/favicon/android-chrome-512x512.png" 
                    class="hoverZoomLink">
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

        <!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 class="news-heading">üî• News</h2>
                <ul id="newsList">
                  <li>
                    <p><span class="news-date">[2025.01]</span> üéâ One paper gets accepted to ICRA 2025.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2024.09]</span> üéì I started <a href="https://www.mccormick.northwestern.edu/robotics/">Master in Robotics (MSR)</a> program at Northwestern University.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2023.09]</span> üíº I started my gap year as a research assistant at <a href="https://zsdonghao.github.io/">PKU-Agibot (Êô∫ÂÖÉÊú∫Âô®‰∫∫) Lab</a>, Peking University.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2023.07]</span> üéâ Two papers get accepted to CICAI 2023, with a <b>Best Demo Award.</b></p>
                  </li>
                  <li>
                    <p><span class="news-date">[2023.07]</span> üéâ One paper gets accepted to ICCV 2023.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2023.06]</span> üéì Get my B.Eng. in Mechanical Engineering at Beijing Univeristy of Chemical Technology.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2022.10]</span> ü¶æ Improved <a href="https://www.kickstarter.com/projects/mdrobotkits/mini-pupper-2-open-source-ros2-robot-kit-for-dreamers">Mini Pupper 2</a> and launched it on Kickstarter.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2022.06]</span> üéâ Published an <a href="https://aws.amazon.com/blogs/robotics/build-and-simulate-a-mini-pupper-robot-in-the-cloud-without-managing-any-infrastructure/">AWS Robotics Blog</a> in deploying applcation changes from cluod-based simulation to real robot hardware.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2022.05]</span> üèÜ We get a <a href="https://github.com/mvyp/.github/blob/main/profile/imgs/2.jpg">Third Prize</a> in ICRA 2022 RoboMaster University Sim2Real Challenge.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2021.10]</span> üíº I started my roboitcs journey as an intern at <a href="https://air.tsinghua.edu.cn/en/index.htm">Institute for Al Industry Research (AIR)</a>, Tsinghua University.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2021.09]</span> ü¶æ Our open-source quadruped robot <a href="https://www.kickstarter.com/projects/mdrobotkits/mini-pupper-open-sourceros-robot-dog-kit/description">Mini Pupper</a> is launched on Kickstarter, and crowd-funded about $500, 000.</p>
                  </li>
                  <li>
                    <p><span class="news-date">[2021.05]</span> üèÜ We get a <a href="https://github.com/mvyp/.github/blob/main/profile/imgs/4.jpg">Second Prize</a> in RoboCup@Home 2021, China.</p>
                  </li>
                </ul>
                <a href="#" id="expandNews" class="expand-link">Show more</a>
              </td>
            </tr>
          </tbody>
        </table>
        <script>
          const newsList = document.getElementById('newsList');
          const expandLink = document.getElementById('expandNews');
          const newsItems = newsList.getElementsByTagName('li');
          const initialDisplay = 5;
          let isExpanded = false;
          function toggleNews() {
            for (let i = initialDisplay; i < newsItems.length; i++) {
              newsItems[i].classList.toggle('hidden');
            }
            isExpanded = !isExpanded;
            expandLink.textContent = isExpanded ? 'Show less' : 'Show more';
          }
          if (newsItems.length > initialDisplay) {
            for (let i = initialDisplay; i < newsItems.length; i++) {
              newsItems[i].classList.add('hidden');
            }
            expandLink.style.display = 'inline-block';
          } else {
            expandLink.style.display = 'none';
          }
          expandLink.addEventListener('click', function(e) {
            e.preventDefault();
            toggleNews();
          });
        </script>

        <!-- <table width="100%" align="center" border="0" cellpadding="20">
          <tbody>
            <tr>
              <td>
                <h2>üßë‚Äçüéì Education</h2>
              </td>
            </tr>
            <tr>
              <td width="75%" valign="center">
                <b>Northwestern University, Evanston, Illinois, U.S.A.</b></br>
                Master of Science, Robotics</br>
                <em style="color:grey;">2024-2025</em></br>
                GPA: 4.00/4.00
              </td>
            </tr>
            <tr>
              <td width="75%" valign="center">
                <b>Beijing University of Chemical Technology, Beijing, China</b></br>
                Bachelor of Engineering, Mechanical Design, Manufacturing and Its Automation</br>
                <em style="color:grey;">2019-2023</em></br>
                GPA: 87.4/100, Ranking: 6/72. Ex-president of <a href="https://github.com/mvyp/"> SIE Robotics Club</a>. Guitar player in the Towers (a student band).
              </td>
            </tr>
          </tbody>
        </table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>üìö Publications</h2>
                  <p>
                    * : Equal contribution; ‚Ä†: Corresponding author(s)
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="icra2025()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icra2025' style="opacity: 0;">
                    </div>
                    <img src='images/feepose.png' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('icra2025').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('icra2025').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Foundation Feature-Driven Online End-Effector Pose Estimation: A Marker-Free and Learning-Free Approach</span>
                  <br>
                  <a href="index.html">Tianshu Wu*</a>, 
                  <a href="https://jiyao06.github.io/">Jiyao Zhang*</a>, 
                  <a href="index.html">Sheldon Liang*</a>, 
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://zsdonghao.github.io/">Hao Dong<sup>&dagger;</sup></a>
                </br>
                  <strong>
                    <em>ICRA, 2025</em>
                  </strong>
                  <br>
                  <a href="https://feepose.github.io/media/FEEPE.pdf">
                    <b>
                      [Paper]
                    </b>
                  </a>
                  <a href="https://feepose.github.io/">
                    <b>
                      [Website]
                    </b>
                  </a>
                </td>
              </tr>

              <tr onmouseout="cicai()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cicai' style="opacity: 0;">
                    </div>
                    <img src='images/cicai.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('m2sim').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('m2sim').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Enhancing Daily Life Through an Interactive Desktop Robotics System</span>
                  <br>
                  <a href="https://mrsecant.github.io/">Yuhang Zheng</a>, 
                  <a href="https://king-bridge.github.io/">Qiyao Wang</a>, 
                  <a href="https://zhongcl-thu.github.io/">Chengliang Zhong<sup>&dagger;</sup></a>, 
                  <a href="index.html">He Liang</a>, 
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="index.html">Yupeng Zheng</a>
                  </br>
                  <strong>
                  <em>CICAI, 2023</em>
                  üèÜ
                  <em style="color: #FF0000;">Best Demo Award</em>
                  </strong>
                  <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-981-99-9119-8_8">
                    <b>
                      [Paper]
                    </b>
                  </a>
                  <a href="https://www.bilibili.com/video/BV1UX4y1n7DJ">
                    <b>
                      [Video]
                    </b>
                  </a>
                  <p>
                    We developed an intelligent desktop operating robot designed to assist 
                    humans in their daily lives by comprehending natural language with large 
                    language models (LLM) and performing a variety of desktop-related tasks.
                  </p>
                </td>
              </tr>

              <tr onmouseout="m2sim()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='m2sim' style="opacity: 0;">
                    </div>
                    <img src='images/m2sim.jpg' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('m2sim').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('m2sim').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Long-term Interactive Driving Simulation: MPC to the Rescue</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://scholar.google.com/citations?user=4PXGeaYAAAAJ">Zhijie Yan</a>, 
                  <a href="https://www.linkedin.com/in/yang-li-2000/">Yang Li</a>, 
                  <a href="https://philipflyg.github.io/">Pengfei Li</a>, 
                  <a href="https://scholar.google.com/citations?user=KlHuj2QAAAAJ">Yifeng Shi</a>, 
                  <a href="index.html">Nairui Luo</a>, 
                  <a href="https://scholar.google.com/citations?user=qCbWxzgAAAAJ">Xu Gao</a>, 
                  <a href="https://scholar.google.com/citations?user=alz2MpAAAAAJ">Yongliang Shi</a>, 
                  <a href="index.html">Pengfei Huang</a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1477.htm">Jiangtao Gong</a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1196.htm">Guyue Zhou</a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Yilun Chen</a>, 
                  <a href="https://hangzhaomit.github.io/">Hang Zhao</a>, 
                  <a href="https://sites.google.com/view/fromandto">Hao Zhao<sup>&dagger;</sup></a>
                  </br>
                  <strong>
                    <em>CICAI, 2023</em>
                  </strong>
                  <br>
                  <a href="data/CICAI_Paper_Camera_Ready.pdf">
                    <b>
                      [Paper]
                    </b>
                  </a>
                  <a href="https://github.com/0nhc/m2sim">
                    <b>
                      [Code]
                    </b>
                  </a>
                  <a href="https://www.youtube.com/watch?v=Xdxto_ivJ_4">
                    <b>
                      [Video]
                    </b>
                  </a>
                  <p>
                    Cooperated with Baidu Apollo and MARS Lab at Tsinghua University. We propose to introduce a tailored Model Predictive Control 
                    (MPC) module as a rescue into the state-of-the art 
                    interactive trajectory prediction model M2I. Notably, our method can effectively address the Out-of-Domain (OOD) 
                    problem in long-term simulation by enforcing a flexible regularization that admits the replayed data, while still 
                    enjoying the diversity of data-driven predictions.
                  </p>
                </td>
              </tr>

              <tr onmouseout="int2()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='int2' style="opacity: 0;">
                    </div>
                    <img src='images/int2.jpg' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('int2').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('int2').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">INT2: Interactive Trajectory Prediction at Intersections</span>
                  <br>
                  <a href="https://scholar.google.com/citations?user=4PXGeaYAAAAJ">Zhijie Yan</a>, 
                  <a href="https://philipflyg.github.io/">Pengfei Li</a>, 
                  <a href="https://scholar.google.com/citations?user=wDMrCnIAAAAJ">Zheng Fu</a>, 
                  <a href="https://daniellli.github.io/">Shaocong Xu</a>, 
                  <a href="https://scholar.google.com/citations?user=alz2MpAAAAAJ">Yongliang Shi</a>, 
                  <a href="https://scholar.google.com/citations?user=_tz64W0AAAAJ">Xiaoxue Chen</a>, 
                  <a href="https://mrsecant.github.io/">Yuhang Zheng</a>, 
                  <a href="https://www.linkedin.com/in/yang-li-2000/">Yang Li</a>, 
                  <a href="index.html">Tianyu Liu</a>, 
                  <a href="https://www.linkedin.com/in/lichuxuan/">Chuxuan Li</a>, 
                  <a href="index.html">Nairui Luo</a>, 
                  <a href="https://scholar.google.com/citations?user=qCbWxzgAAAAJ">Xu Gao</a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Yilun Chen</a>, 
                  <a href="index.html">Zuoxu Wang</a>, 
                  <a href="https://scholar.google.com/citations?user=KlHuj2QAAAAJ">Yifeng Shi</a>, 
                  <a href="index.html">Pengfei Huang</a>, 
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1012/1219.htm">Jirui Yuan</a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1477.htm">Jiangtao Gong</a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1196.htm">Guyue Zhou</a>, 
                  <a href="https://hangzhaomit.github.io/">Hang Zhao</a>, 
                  <a href="https://sites.google.com/view/fromandto">Hao Zhao<sup>&dagger;</sup></a>
                  </br>
                  <strong>
                    <em>ICCV, 2023</em>
                  </strong>
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_INT2_Interactive_Trajectory_Prediction_at_Intersections_ICCV_2023_paper.pdf">
                    <b>
                      [Paper]
                    </b>
                  </a>
                  <a href="https://github.com/AIR-DISCOVER/INT2">
                    <b>
                      [Code]
                    </b>
                  </a>
                  <a href="https://www.youtube.com/watch?v=KNkuakDvgVc">
                    <b>
                      [Video]
                    </b>
                  </a>
                  <p>
                    We present a large-scale interactive trajectory prediction dataset named INT2 for INTeractive 
                    trajectory prediction at INTersections. INT2 includes 612,000 scenes, each lasting 1 minute, 
                    containing up to 10,200 hours of data. We benchmark the best open-sourced interactive trajectory 
                    prediction method on INT2 and Waymo Open Dataset, under in-domain and cross-domain settings.
                  </p>
                </td>
              </tr>

              <tr onmouseout="aws_blog_mini_pupper()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='aws_blog_mini_pupper' style="opacity: 0;">
                    </div>
                    <img src='images/aws_blog_mini_pupper.jpg' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('aws_blog_mini_pupper').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('aws_blog_mini_pupper').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Build and simulate a Mini Pupper robot in the cloud without managing any infrastructure</span>
                  <br>
                  <a href="https://www.linkedin.com/in/bingjiao-yu?originalSubdomain=cn">Bingjiao Yu</a>, 
                  <a href="https://www.linkedin.com/in/afreez-gan-b606615/?originalSubdomain=hk">Afreez Gan</a>, 
                  <a href="https://www.linkedin.com/in/hansenmattk/">Matt Hansen</a>, 
                  <a href="https://www.linkedin.com/in/xiao-yang-zhu-69967414/?originalSubdomain=cn">Xiaoyang Zhu</a>, 
                  <a href="index.html"><b>Zhengxiao Han</b></a>
                  </br>
                  <strong>
                  <em>AWS Robotics Blog, 2022</em>
                  </strong>
                  <br>
                  <a href="https://aws.amazon.com/blogs/robotics/build-and-simulate-a-mini-pupper-robot-in-the-cloud-without-managing-any-infrastructure/">
                    <b>
                      [Blog]
                    </b>
                  </a>
                  </a>
                  <p>  
                    In this blog, we highlight how AWS RoboMaker simplifies robotics simulation. Using the Mini Pupper robot, 
                    we demonstrate how developers can build and test ROS-based navigation applications in a managed cloud environment. 
                    AWS Cloud9 streamlines development, while AWS RoboMaker enables scalable, parallel testing. This cloud-based solution makes 
                    robotics development more accessible, accelerating workflows and reducing infrastructure management.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>ü¶æ Projects</h2>
                  <p>
                    I have always been working on Robotics.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="wbcd()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='wbcd' style="opacity: 0;">
                    </div>
                    <video width="320" height="180" autoplay loop muted>
                      <source src="videos/wbcd.mp4" type="video/mp4">
                    </video>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('wbcd').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('wbcd').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">ICRA 2025 What Bimanuals Can Do (WBCD) Challenge</span>
                  <br>
                  <a href="https://ericlee0224.github.io/">Weize Li</a>,
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://davidlxu.github.io/">Lixin Xu</a>, 
                  <a href="https://changerc77.github.io/">Xiangyu Chen</a>, 
                  <a href="https://harrisonbounds.github.io/">Harrison Bounds</a>, 
                  <a href="index.html">Chenrui Zhang</a>, 
                  <a href="https://scholar.google.com/citations?user=RYKMFp4AAAAJ&hl=en">Yifan Xu</a>, 
                  </br>
                  <strong>
                    <em>ICRA, 2025</em>
                    üèÜ
                    <em style="color: #FF0000;">The First Place</em>
                  </strong>
                  <br>
                  <a href="https://wbcd.live">
                    <b>
                      [Website]
                    </b>
                  </a>
                  <p>
                    The ICRA 2025 WBCD Challenge is a hackathon organized by Google DeepMind, the Robotics and AI Institute (RAI), 
                    and other leading partners. This competition benchmarks real-world bimanual tasks across three 
                    tracks‚Äîpacking logistics, scientific experiments, and table operations‚Äîpushing robotics from lab 
                    demos to billion-dollar market potential.
                    In the Table Operations track, we tackled a series of demanding tasks under strict 
                    requirements for speed, precision, and reliability: unfolding a tablecloth (deformable-object manipulation), 
                    placing a pizza onto the table (pick-and-place), and opening and closing a food storage box.
                  </p>
                </td>
              </tr>

              <!-- <tr onmouseout="pose_estimation()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='pose_estimation' style="opacity: 0;">
                    </div>
                    <img src='images/pose_estimation.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('pose_estimation').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('pose_estimation').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Point Cloud Classification</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://wengmister.github.io/">Zhengyang (Kris) Weng</a>,
                  <a href="https://benbenyamin.github.io/">Ben Benyamin</a>
                  </br>
                  <strong>
                    <em>Final Project for <a href="https://www.mccormick.northwestern.edu/artificial-intelligence/curriculum/descriptions/msai-437.html">MSAI437: Deep Learning</a></em>
                  </strong>
                  <br>
                  <a href="https://github.com/msr-in-msai/msai-349-final-project">
                    <b>
                      [Code]
                    </b>
                  </a>
                  <p>
                    We re-implemented a 6D pose estimation pipeline (<a href="https://github.com/j96w/DenseFusion">DenseFusion</a>)
                  </p>
                </td>
              </tr> -->

              <tr onmouseout="northwestern_humanoid()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='northwestern_humanoid' style="opacity: 0;">
                    </div>
                    <!-- <img src='images/northwestern_humanoid.gif' width=200%/> -->
                    <video width="320" height="180" autoplay loop muted>
                      <source src="videos/northwestern_humanoid.mp4" type="video/mp4">
                    </video>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('northwestern_humanoid').style.opacity = "1";
                    }
    
                    function bog_stop() {
                      document.getElementById('northwestern_humanoid').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Northwestern Humanoid</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>
                  </br>
                  <strong>
                    <em>ME499: Independent Project</em>
                  </strong>
                  <br>
                  <a href="https://docs.google.com/presentation/d/1MNszr2vIThjyHmdqRFD0j22BspOMXgXHbe75w9jpFPk/edit?usp=sharing">
                    <b>
                      [Slides]
                    </b>
                  </a>
                  <!-- <a href="https://github.com/0nhc/opus_rl/tree/main/journal">
                    <b>
                      [Journal]
                    </b>
                  </a> -->
                  <a href="https://github.com/0nhc/opus_rl">
                    <b>
                      [RL Code]
                    </b>
                  </a>
                  <a href="https://github.com/0nhc/opus_control">
                    <b>
                      [ROS 2 Code]
                    </b>
                  </a>
                  <p>
                    I independently developed a medium-sized humanoid robot from scratch. On the mechanical design front, 
                    I introduced an offset angle to the pelvis and manufactured the hardware using 3D printing. For the locomotion algorithm, 
                    I created a set of reward functions and implemented PPO reinforcement learning using the <a href="https://github.com/leggedrobotics/rsl_rl">rsl_rl</a> library and <a href="https://genesis-world.readthedocs.io/en/latest/user_guide/overview/what_is_genesis.html">Genesis</a> simulator to enable walking. 
                    In summary, over this 10-week project, I built the robot's hardware, integrated its motors and sensors with ROS, 
                    and successfully simulated its walking using an RL policy.
                  </p>
                </td>
              </tr>

              <tr onmouseout="rds2025_2n()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='rds2025_2n' style="opacity: 0;">
                    </div>
                    <img src='images/rds2025_2n.gif' width=200%/>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('rds2025_2n').style.opacity = "1";
                    }
    
                    function bog_stop() {
                      document.getElementById('rds2025_2n').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Tendon-Driven Dexterous Hand Basics: 2N Finger</span>
                  <br>
                  <a href="https://www.linkedin.com/in/klaudon/">Konrad Laudon</a>, 
                  <a href="https://www.linkedin.com/in/eliana-storkamp-7771ab22a/">Eliana Storkamp</a>, 
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://www.linkedin.com/in/cwseif/">Charlie Seifert</a>, 
                  <a href="https://www.linkedin.com/in/sairam-umakanth/">Sairam Umakanth</a>, 
                  <a href="https://www.linkedin.com/in/yifei-chen-a89498310/">Yifei Chen</a>,
                  <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/lynch-kevin.html">Kevin Lynch</a>
                  </br>
                  <strong>
                    <em>Warm-up Project for <a href="https://www.mccormick.northwestern.edu/mechanical/academics/courses/descriptions/472-1-robot-design-studio.html">ME472: Robot Design Studio</a></em>
                  </strong>
                  <br>
                  <a href="https://github.com/NU-RDS/2n-codebase">
                    <b>
                      [Code]
                    </b>
                  </a>
                  <a href="https://absrat.com/rds2025_team2n">
                    <b>
                      [Documentation]
                    </b>
                  </a>
                  <p>
                    This warm-up project for Robot Design Studio explores the concept of a dexterous 2N finger. 
                    Built with a 3D-printed structure, tendon-driven actuation, and ODrive Pro-controlled brushless motors, 
                    it uses custom CAN and serial protocols for smooth communication between the Teensy, ODrive, and laptop. 
                    We verified the kinematics by displaying joint states in the GUI and Rviz, integrated a ROS 2 interface 
                    for future expansion, and demonstrated PD control.
                    One more thing, we over-toasted one ODrive Pro (1*$229), two Teensy 4.1 boards(2*$31.5), and four MCP2561 chips(4*$1.28)!
                  </p>
                </td>
              </tr>

              <tr onmouseout="pcd_classify()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='pcd_classify' style="opacity: 0;">
                    </div>
                    <img src='images/pcd_classify.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('pcd_classify').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('pcd_classify').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Point Cloud Classification</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://wengmister.github.io/">Zhengyang (Kris) Weng</a>,
                  <a href="https://benbenyamin.github.io/">Ben Benyamin</a>
                  </br>
                  <strong>
                    <em>Final Project for <a href="https://www.mccormick.northwestern.edu/artificial-intelligence/curriculum/descriptions/msai-349.html">MSAI349: Machine Learning</a></em>
                  </strong>
                  <br>
                  <a href="https://github.com/msr-in-msai/msai-349-final-project">
                    <b>
                      [Code]
                    </b>
                  </a>
                  <p>
                    We developed a PyTorch object classification model that processes point cloud data enriched with RGB 
                    information using a modified PointNet architecture. Our approach leverages a realistic dataset generated 
                    in Isaac Sim from <a href="https://omniobject3d.github.io/">OmniObject3d</a> object models, capturing RGB-D 
                    images and segmentation labels for five classes 
                    (apple, banana, bottle, bowl, cup). With a four-layer MLP, max pooling, and transformation networks 
                    addressing order and pose variations, our model achieves 70% testing accuracy and outperforms traditional KNN methods.
                  </p>
                </td>
              </tr>


              <tr onmouseout="doodle_droid()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='doodle_droid' style="opacity: 0;">
                    </div>
                    <img src='images/doodle_droid.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('doodle_droid').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('doodle_droid').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Doodle Droid <b style="color: #FF0000;">(Reposted by Franka Robotics)</b></span>
                  <br>
                  <a href="https://ykechriotis.github.io/">Yanni Kechriotis</a>, 
                  <a href="https://cappuccinopanda.github.io/website/">David Matthews</a>, 
                  <a href="https://github.com/HarrisonBounds">Harrison Bounds</a>, 
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="index.html">Christian Tongate</a>, 
                  <a href="https://robotics.northwestern.edu/people/profiles/faculty/elwin-matt.html">Matthew Elwin</a>
                  </br>
                  <strong>
                    <em>Final Project for <a href="https://nu-msr.github.io/ros_notes/index.html">ME495: Embedded Systems in Robotics</a></em>
                  </strong>
                  <br>
                  <a href="https://github.com/HarrisonBounds/DoodleDroid">
                    <b>
                      [Code]
                    </b>
                  </a>
                  <a href="https://www.linkedin.com/posts/harrison-bounds_introducing-doodle-droid-a-portrait-drawing-activity-7293646260731883520-NiNA?utm_source=share&utm_medium=member_desktop&rcm=ACoAADc8j2YBWU41NvujlwJ5927HA9-7dKcrazM">
                    <b>
                      [LinkedIn Post]
                    </b>
                  </a>
                  <p>
                    A franka robot arm is used to draw portraits as line art. 
                    Users can take a photo of themselves or others which the robot will convert to pen strokes and draw 
                    them on a paper detected and localized using april tags. My primary role is to develop the MoveIt 2 API library. 
                    My secondary role is to integrate the code.
                  </p>
                </td>
              </tr>

              <tr onmouseout="msr_hackathon()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='msr_hackathon' style="opacity: 0;">
                    </div>
                    <img src='images/msr_hackathon.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('msr_hackathon').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('msr_hackathon').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Northwestern University MSR Hackathon</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://robotics.northwestern.edu/people/profiles/faculty/elwin-matt.html">Matthew Elwin</a>
                  <p>
                    My MSR journey at Northwestern University kicked off with an intense two-week hackathon. We firstly reviewed BFS, DFS and RRT path planning algorithms. 

                    The most complex challenge was designing a robotic grasping system. I used HSV filtering to create a mask for detecting a purple pen, and by leveraging the camera's intrinsic matrix and aligned depth image, I calculated the pen's 3D position. To handle camera movements, I used an ArUco tag to dynamically calculate the transformation between the camera and robotic arm, making my system robust to any accidental camera shifts. At last I can generate a 6D grasping pose for the robotic arm.
                  </p>
                </td>
              </tr>

              <tr onmouseout="active_perception()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='active_perception' style="opacity: 0;">
                    </div>
                    <img src='images/active_perception.jpg' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('active_perception').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('active_perception').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Synthetic Grasping Dataset Generation</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://jiyao06.github.io/">Jiyao Zhang</a>, 
                  <a href="https://zsdonghao.github.io/">Hao Dong</a>
                  <p>
                    I generated a synthetic dataset by Utilizing Isaac Sim and <a href="https://omniobject3d.github.io/">OmniObject3d</a>. 
                    By extracting the model's pointcloud, I used RANSAC to fit the planes, then calculate the normal vectors, and finally 
                    used convex hulls to find areas that can place other objects. With this pipeline, I generated more than 300,000 data 
                    samples including RGB, Depth, and Segmentation information for training a grasping policy. The whole data generation 
                    process was done by me independently.
                  </p>
                </td>
              </tr>

              <tr onmouseout="dex_7dof()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dex_7dof' style="opacity: 0;">
                    </div>
                    <img src='images/dex_7dof.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('dex_7dof').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('dex_7dof').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">7-DOF Humanoid Robotic Arm with Tactile-enabled Dexterous Hand</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://kingchou007.github.io">Jinzhou Li</a>, 
                  <a href="https://ceca.pku.edu.cn/en/people_/faculty_/tao_wang/">Tao Wang</a>, 
                  <a href="https://zsdonghao.github.io/">Hao Dong</a>
                  <p>
                  Constructed a 7-DOF humanoid robotic arm with dexterous hands equipped with tactile sensors for data collection, 
                  and interfaced it with ROS Control and MoveIt. All work was done by me independently.
                  We will work on encoding visual, tactile (not vision-based) and joint signals for imitation learning and deploy learned policies on real robots.
                  </p>
                </td>
              </tr>


              <tr onmouseout="tactile_sensor()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='tactile_sensor' style="opacity: 0;">
                    </div>
                    <img src='images/bi_9dtact.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('tactile_sensor').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('tactile_sensor').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Vision-based Tactile Sensor</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://jiyao06.github.io/">Jiyao Zhang</a>, 
                  <a href="https://zsdonghao.github.io/">Hao Dong</a>
                  <p>
                    Developed a binocular version of <a href="https://linchangyi1.github.io/9DTact/">9DTact</a>, 
                    a vision-based tactile sensor for robotic grasping. I independently designed all mechanical 
                    and electrical components‚Äîincluding the 3D-printed housing and custom PCB. Although the design 
                    showed promise, camera lens issues eventually rendered this version obsolete.
                  </p>
                </td>
              </tr>

              <tr onmouseout="full_stack_6dof()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='full_stack_6dof' style="opacity: 0;">
                    </div>
                    <img src='images/full_stack_6dof.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('full_stack_6dof').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('full_stack_6dof').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Full-stack 6-DOF robotic arm development</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://jiyao06.github.io/">Jiyao Zhang</a>, 
                  <a href="https://zsdonghao.github.io/">Hao Dong</a>	
                  <p>
                  Achived hardware design of a new 6-DOF robotic arm, 
                  and interfaced all hardware under ROS Control and MoveIt. Interfaced the robotic arm with 
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Graspness_Discovery_in_Clutters_for_Fast_and_Accurate_Grasp_Detection_ICCV_2021_paper.pdf">GSNet</a> and achived a grasping demo. All work was done by me independently.
                  </p>
                </td>
              </tr>

              <tr onmouseout="full_stack_arm_and_gripper()" onmouseover="bog_start()"></tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='full_stack_arm_and_gripper' style="opacity: 0;">
                    </div>
                    <img src='images/full_stack_arm_and_gripper.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('full_stack_arm_and_gripper').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('full_stack_arm_and_gripper').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle">Full-stack 6-DOF robotic arm and gripper development</span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://jiyao06.github.io/">Jiyao Zhang</a>, 
                  <a href="https://zsdonghao.github.io/">Hao Dong</a>	
                  <p>
                  Achived hardware design of a 6-DOF robotic arm and a 2-finger adaptive parallel gripper 
                  (same capability as <a href="https://robotiq.com/products/2f85-140-adaptive-robot-gripper">Robotiq 2f-85 Gripper</a>).
                  Constructed a robotic arm SDK with CMake, and interfaced the lib with ROS and Isaac Sim.
                  </p>
                </td>
              </tr>

              <tr onmouseout="bimanual_demonstration()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='bimanual_demonstration' style="opacity: 0;">
                    </div>
                    <img src='images/bimanual_demonstration.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('bimanual_demonstration').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('bimanual_demonstration').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle"><a href="https://www.bilibili.com/video/BV1iN411t7eb">Bimanual Demonstration System</a></span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://github.com/ShanningZhuang">Shanning Zhuang</a>, 
                  <a href="http://robotics-tongji.org/members/zihangchen">Zihang Chen</a>, 
                  <a href="https://github.com/GDDG08">Zihan Zhuang</a>, 
                  <a href="http://www.iden.cn/home/active.NewYouzhan/workinfo?sc=yx&id=13012">Ximing Wang</a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1477.htm">Jiangtao Gong</a>
                  <p>
                  Inspired by ALOHA, we proposed an improved bimanual demonstration system. 
                  Constructed the SDK using C++ and CMake. Utilized KDL to solve kinematic and dynamic problems, 
                  then achived gravity compensation based on inverse dynamics. 
                  Designed a parallel two-finger gripper based on rack-gear structures. 
                  </p>
                </td>
              </tr>

              <tr onmouseout="mini_pupper_2()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mini_pupper_2' style="opacity: 0;">
                    </div>
                    <img src='images/mini_pupper_2.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('mini_pupper_2').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('mini_pupper_2').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle"><a href="https://www.kickstarter.com/projects/336477435/mini-pupper-2-open-source-ros2-robot-kit-for-dreamers">Mini Pupper 2: Open-Source Quadruped Robot Dog Kit</a></span>
                  <br>
                  <a href="www.linkedin.com/in/afreez-gan-b606615">Afreez Gan</a>, 
                  <a href="index.html">Lily Wang</a>, 
                  <a href="index.html">Jian Song</a>, 
                  <a href="https://www.linkedin.com/in/marcin-pryli%C5%84ski-a79190249/">Marcin Prylinski</a>, 
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://github.com/Ericsii">Yunlong Feng</a>
                  <p>
                    Developed all the ROS 2 (Humble) software suite including Locomotion, SLAM, Navigation, and CV functions. 
                    Deployed Cartographer with an Extended Kalman Filter (EKF) to fuse IMU data and LiDAR odometry, 
                    enhancing the robostness and accuracy of Mini Pupper's localization system.
                  </p>
                </td>
              </tr>

              <tr onmouseout="multi_robot()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='multi_robot' style="opacity: 0;">
                    </div>
                    <img src='images/multi_robot.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('multi_robot').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('multi_robot').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle"><a href="https://www.youtube.com/watch?v=kmLZ6OmiqrY">Cooperative Multi-Robot System</a></span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="index.html">Jingtian Deng</a>, 
                  <a href="https://linden713.github.io/">Chenghao Lin</a>, 
                  <a href="https://changerc77.github.io/">Xiangyu Chen</a>, 
                  <a href="https://www.linkedin.com/in/tian-ao-ren-2a0349220/">Tianao Ren</a>, 
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1196.htm">Guyue Zhou</a>
                  <p>
                    Utilized Pure Pursuit algorithm for multi-robot formation control, which was tested in both real world and Isaac Sim. 
                    Utilized an EKF-based 2D LiDAR localization system as the odometry. 
                    Utilized KDL for solving kinematic problems of our own 6-DOF robotic arm, 
                    and combined it with the odometry, achieving Chiken-Head Mode. 
                    Combined all the functions above, and constructed a multi-robot cooperative system to carry a box in Isaac Sim.
                  </p>
                </td>
              </tr>

              <tr onmouseout="rmus()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='rmus' style="opacity: 0;">
                    </div>
                    <img src='images/rmus.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('rmus').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('rmus').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle"><a href="https://youtu.be/7V5SHpF0PHM">ICRA 2022 RoboMaster University Sim2real Challenge</a></span>
                  <br>
                  <a href="index.html"><b>Zhengxiao Han</b></a>, 
                  <a href="https://linden713.github.io/">Chenghao Lin</a>, 
                  <a href="https://www.linkedin.com/in/tian-ao-ren-2a0349220/">Tianao Ren</a>, 
                  <a href="index.html">Luoji Zhu</a>, 
                  <a href="index.html">Haitao Rao</a>
                  <p>
                    Deployed an Extended Kalman Filter (EKF) alongside an omnidirectional motion model for state estimation using sensor data, including IMU and odometry.
                    Utilized A* algorithm for global path planning and Timed Elastic Band (TEB) algorithm for local path planning. 
                    Utilized ArUco library for detecting boxes' poses.
                  </p>
                </td>
              </tr>

              <tr onmouseout="mini_pupper()" onmouseover="bog_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mini_pupper' style="opacity: 0;">
                    </div>
                    <img src='images/mini_pupper.gif' width=200%>
                  </div>
                  <script type="text/javascript">
                    function bog_start() {
                      document.getElementById('mini_pupper').style.opacity = "1";
                    }

                    function bog_stop() {
                      document.getElementById('mini_pupper').style.opacity = "0";
                    }
                    bog_stop()
                  </script>
                </td>
                <td style="padding:35px;width:75%;vertical-align:middle">
                  <span class="papertitle"><a href="https://www.kickstarter.com/projects/336477435/mini-pupper-open-sourceros-robot-dog-kit">Mini Pupper: Open-Source Quadruped Robot Dog Kit</a></span>
                  <br>
                  <a href="www.linkedin.com/in/afreez-gan-b606615">Afreez Gan</a>, 
                  <a href="https://www.linkedin.com/in/marcin-pryli%C5%84ski-a79190249/">Marcin Prylinski</a>, 
                  <a href="https://twitter.com/tomato5356">Xiongshi Xu</a>, 
                  <a href="index.html"><b>Zhengxiao Han</b></a>
                  <p>
                    Inspired by Stanford's open-source quadruped robot <a href="https://pupper.readthedocs.io/en/latest/">Pupper</a>, I designed my own mechanical hardware, mainly improved its leg structures. 
                    After that, I was contacted by Mini Pupper's team and joined them. We developed the first product together. 
                    I independently developed all the ROS software suite including Locomotion, SLAM, Navigation, and CV functions. 
                    We crowd-funded $500,000 on Kickstarter when I was a junior.
                  </p>
                </td>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:30%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px;text-align:right;font-size:small">
                  <br>
                  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=p9qCYK-rJEcF_FBhZsQbjU7RvhXNquQf7q7s7NNyOiA&cl=ffffff&w=a"></script>
                  <p style="text-align:center;font-size:small;">
                    Design and source code from <a href="https://github.com/jonbarron/website">Jon Barron's website</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </table>
  </body>
</html>
